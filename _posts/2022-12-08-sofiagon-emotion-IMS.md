---
layout: post
title: "Interactive Music Systems, Communication and Emotion"
date: 2022-12-09 10:00:00 +0200
categories: interactive-music
author: Sofía González
image: /assets/image/2022_05_12_sofiagon_gesture.jpg
keywords: ims, emotion, psychology, motion capture
excerpt: "There are many takes on gesticulation and its meanings, however, I wanted to take the time to delimit what a gesture is, possible categories and gesturing patters."
---

A definition for Interactive music systems (IMSs) is "those [systems] whose behavior changes in response to musical input" [1]. The more technology advances and becomes more accessible, the easier it is for everyone to get involved and create new ways to compose and interact with music. As artists do with every other art form: creating to communicate.

<figure style="float: none">
   <img src="/assets/image/2022_12_09_sofiagon_caricature.jpg" alt="" title="by CDD20 in pixabay" width="auto" />
</figure>

Communication is an important aspect I like to contemplate when considering the possibilities of building new instruments. Specially the communication of emotions. Part of what defines a IMS as such, is the ability to be expressive and/or to amplify expression. The expression of creativity and the expression of emotion.
If we look at IMSs from this perspective of emotion, we can apply psychology to IMS design in order to more effectively create a system that will express or even induce certain emotions purposefully.
In fact, there are already some IMSs that have been created for this same purpose: inducing specific emotions.

<figure style="float: none">
   <img src="/assets/image/2022_12_09_sofiagon_caricature_2.jpg" alt="" title="by CDD20 in pixabay" width="auto" />
</figure>

One example of this is the SiMS, a situated interactive music system created to explore the relationship between physiology, brain activity, musical features and emotion [2]. In order to do this, the authors proposed a software framework "based on solid psychophysiological and psychoacoustical principles" [2].
To put some examples, they used the heart rate data to measure for attention and arousal (a dimension of affect). Apparently, attention evokes the deceleration of heart rate, while arousal evokes the opposite effect. Other interesting examples are:
- Increases in tempo or volume are correlated with valence and arousal.
- Shortening of articulations correlates with increased valence.
- Brightness and noisiness (or lack thereof) correlate with arousal.

So they took all of this knowledge and created a psychologically based IMS that generates music to induce specific emotions. If they wanted to induce a calm and relaxed emotion (which would equal high valence and low arousal) they would modify the already mentioned elements so that the music generated met the required criteria. This translates into the music generator being set to low brightness and noisiness, low tempo and volume and long articulations.

&nbsp;
Apart from the SiMS, there are other similar IMSs such as the EmoteControl [3] or the Emotion Box [4] that are, as well, based on psychology to be as effective as possible with expression of emotions.

<figure style="float: none">
   <img src="/assets/image/2022_12_09_sofiagon_heartrate.png" alt="" title="by Dung Quách in pixabay" width="auto" />
</figure>

&nbsp;

As a singer, I use music as a way to express my emotions, whether they be positive or negative. That is one of the main reasons why music is so important to me. It allows me to express feelings that, otherwise, I wouldn't know how to express. That's the reason why being able to interact with instruments/systems that enhance my ability to be expressive is so appealing to me.
However, and as the authors mention in their paper [2], these emotion-centered IMS can be more useful than just that. These systems can become efficient tools in music therapy and help people. Because, in the end, creating is always for the sake of people. Whether it helps its creator or the people it affects as an audience, art is always human-centered.

&nbsp;

&nbsp;

&nbsp;

&nbsp;

# References
[1] R. Rowe, "Chapter 1: Interactive Music Systems" in *Interactive Music Systems*, 1993. https://wp.nyu.edu/robert_rowe/text/interactive-music-systems-1993/ [Accessed Dec. 09, 2022]
[2] S. Le Groux and P. Verschure, "Situated Interactive Music System: Connecting Mind and Body Through Musical Interaction" in *Proceedings of the 2009 International Computer Music Conference, ICMC*, 2009, San Francisco, CA, USA. https://www.researchgate.net/publication/209435990_Situated_Interactive_Music_System_Connecting_Mind_and_Body_Through_Musical_Interaction/stats
[3] A.M. Grimaud and T. Eerola, "EmoteControl: an interactive system for real-time control of emotional expression in music." *Pers Ubiquit Comput 25, 677–689* (2021). https://doi.org/10.1007/s00779-020-01390-7
[4] K. Zheng, R. Meng, C. Zheng, X. Li, J. Sang, J. Cai, J. Wang and X. Wang, "EmotionBox: A music-element-driven emotional music generation system based on music psychology",  *Front. Psychol.* 13:841926. doi: 10.3389/fpsyg.2022.841926
